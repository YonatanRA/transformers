{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fe9eb90",
   "metadata": {},
   "source": [
    "# 05 - Tranformers desde cero\n",
    "\n",
    "![transformers](../images/transformers.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cc2584",
   "metadata": {},
   "source": [
    "Para construir nuestro modelo Transformer, seguiremos estos pasos:\n",
    "\n",
    "1. Importar las librerías y módulos necesarios\n",
    "2. Definir los componentes básicos: Multi-Atención, redes feed-forward en función de la posición, codificación posicional.\n",
    "3. Construir las capas de codificación y descodificación\n",
    "4. Combinar las capas de codificación y descodificación para crear el modelo Transformer completo.\n",
    "5. Preparación de datos de muestra\n",
    "6. Entrenar el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e38aad",
   "metadata": {},
   "source": [
    "# 1 - Importar las librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10097021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import math\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36215e58",
   "metadata": {},
   "source": [
    "# 2.1 - Multi-Atención\n",
    "\n",
    "![transformers_2](../images/transformers_2.png)\n",
    "\n",
    "El mecanismo de atención multicabezal calcula la atención entre cada par de posiciones de una secuencia. Consta de varias \"cabezas de atención\" que captan distintos aspectos de la secuencia de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec55d1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        \n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "    \n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        \n",
    "    def combine_heads(self, x):  \n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        \n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df48666e",
   "metadata": {},
   "source": [
    "El código de MultiHeadAttention inicializa el módulo con parámetros de entrada y capas de transformación lineal. Calcula las puntuaciones de atención, remodela el tensor de entrada en múltiples cabezas y combina las salidas de atención de todas las cabezas. El método feed-forward calcula la auto-atención de varias cabezas, lo que permite al modelo centrarse en algunos aspectos diferentes de la secuencia de entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80356b19",
   "metadata": {},
   "source": [
    "# 2.2 - Redes Feed-Forward en función de la posición\n",
    "\n",
    "![transformers_exp_8](../images/transformers_exp_8.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff74ee97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, d_ff):\n",
    "        \n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53e90be",
   "metadata": {},
   "source": [
    "La clase PositionWiseFeedForward extiende nn.Module de PyTorch e implementa una red feed-forward de posición. La clase se inicializa con dos capas de transformación lineal y una función de activación ReLU. El método forward aplica estas transformaciones y la función de activación secuencialmente para calcular la salida. Este proceso permite al modelo tener en cuenta la posición de los elementos de entrada al realizar predicciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b66048",
   "metadata": {},
   "source": [
    "# 2.3 - Positional Encoding\n",
    "\n",
    "![transformers_3](../images/transformers_3.png)\n",
    "\n",
    "La codificación posicional se utiliza para inyectar la información de posición de cada token (palabra) en la secuencia de entrada. Utiliza funciones seno y coseno de distintas frecuencias para generar la codificación posicional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ee886d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b620e2",
   "metadata": {},
   "source": [
    "La clase PositionalEncoding se inicializa con los parámetros de entrada d_model y max_seq_length, creando un tensor para almacenar valores de codificación posicional. La clase calcula valores de seno y coseno para índices pares e impares, respectivamente, basándose en el factor de escala div_term. El método forward calcula la codificación posicional añadiendo los valores de codificación posicional almacenados al tensor de entrada, permitiendo al modelo capturar la información de posición de la secuencia de entrada.\n",
    "\n",
    "Ahora, construiremos las capas Encoder y Decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b520d4",
   "metadata": {},
   "source": [
    "# 3.1 - Encoder\n",
    "\n",
    "![transformers_4](../images/transformers_4.webp)\n",
    "\n",
    "Una capa encoder consta de una capa de multi-atención, una capa feed-forward de posición y dos capas de normalización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fd742b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        \n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68c4089",
   "metadata": {},
   "source": [
    "La clase EncoderLayer se inicializa con parámetros de entrada y componentes, incluyendo un módulo MultiHeadAttention, un módulo PositionWiseFeedForward, dos módulos de normalización de capas y una capa de dropout. Los métodos feed-forward calculan la salida de la capa codificadora aplicando auto-atención, añadiendo la salida de atención al tensor de entrada y normalizando el resultado. A continuación, calcula la salida feed-forward en función de la posición, la combina con la salida de auto-atención normalizada y normaliza el resultado final antes de devolver el tensor procesado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7552aa9c",
   "metadata": {},
   "source": [
    "# 3.2 - Decoder\n",
    "\n",
    "![transformers_5](../images/transformers_5.webp)\n",
    "\n",
    "Una capa decoder consta de dos capas de multi-atención, una capa feed-forward de posición y tres capas de normalización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "757fd876",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        \n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fe39eb",
   "metadata": {},
   "source": [
    "La clase DecoderLayer se inicializa con parámetros de entrada y componentes como los módulos MultiHeadAttention para auto-atención enmascarada y atención cruzada, un módulo PositionWiseFeedForward, tres módulos de normalización de capas y una capa de dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8be6f4",
   "metadata": {},
   "source": [
    "El método forward calcula la salida de la capa decodificadora realizando los siguientes pasos:\n",
    "\n",
    "1. Calcular la salida de auto-atención enmascarada y añadirla al tensor de entrada, seguido de un dropout y normalización de la capa.\n",
    "2. Calcular la salida de atención cruzada entre las salidas del decoder y del encoder y añadirla a la salida de auto-atención enmascarada normalizada, seguida de un dropout y una normalización de la capa.\n",
    "3. Calcular la salida feed-forward en función de la posición y combinarla con la salida normalizada de atención cruzada, seguida de un dropout y normalización de capa.\n",
    "4. Devuelve el tensor procesado.\n",
    "\n",
    "Estas operaciones permiten al decodificador generar secuencias objetivo basadas en la entrada y la salida del codificador.\n",
    "\n",
    "Ahora, combinemos las capas Encoder y Decoder para crear el modelo Transformer completo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1addc304",
   "metadata": {},
   "source": [
    "# 4 - Transformer\n",
    "\n",
    "![transformers_6](../images/transformers_6.webp)\n",
    "\n",
    "\n",
    "Todo junto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8091e91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, \n",
    "                 num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        \n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8650da",
   "metadata": {},
   "source": [
    "La clase Transformer combina los módulos definidos anteriormente para crear un modelo Transformer completo. Durante la inicialización, el módulo Transformer configura los parámetros de entrada e inicializa varios componentes, incluyendo capas de embebido para secuencias de origen y destino, un módulo PositionalEncoding, módulos EncoderLayer y DecoderLayer para crear capas apiladas, una capa lineal para proyectar la salida del decoder y una capa de dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49286758",
   "metadata": {},
   "source": [
    "El método generate_mask crea máscaras binarias para las secuencias de origen y destino con el fin de ignorar los tokens de relleno y evitar que el decoder atienda a tokens futuros. El método forward calcula la salida del modelo Transformer a través de los siguientes pasos:\n",
    "\n",
    "1. Generar máscaras de origen y destino utilizando el método generate_mask.\n",
    "2. Calcular los embebidos de origen y destino, y aplicar la codificación posicional y el dropout.\n",
    "3. Procesar la secuencia de origen a través de capas codificadoras, actualizando el tensor enc_output.\n",
    "4. Procesar la secuencia de destino a través de capas decodificadoras, utilizando enc_output y máscaras, y actualizando el tensor dec_output.\n",
    "5. Aplicar la capa de proyección lineal a la salida del decodificador, obteniendo logits de salida.\n",
    "\n",
    "Estos pasos permiten al modelo Transformer procesar secuencias de entrada y generar secuencias de salida basándose en la funcionalidad combinada de sus componentes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886da400",
   "metadata": {},
   "source": [
    "# 5 - Datos de muestra\n",
    "\n",
    "En este ejemplo, crearemos un conjunto de datos de juguete con fines de demostración. En la práctica, utilizaremos un conjunto de datos mayor, preprocesaremos el texto y crearemos correspondencias de vocabulario para las lenguas de origen y de destino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acd7c361",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = 5000\n",
    "tgt_vocab_size = 5000\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 100\n",
    "dropout = 0.1\n",
    "\n",
    "# Generate random sample data\n",
    "src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
    "tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc424414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4913, 3021, 1639,  ..., 3907, 1389, 1234],\n",
       "        [1757, 2977, 4557,  ...,  157, 3450,  389],\n",
       "        [ 669, 2238, 4430,  ..., 4702,  906, 3132],\n",
       "        ...,\n",
       "        [4185, 2745, 2587,  ..., 2658, 1461, 2338],\n",
       "        [1628,  341, 4858,  ..., 1719,  445,  113],\n",
       "        [1657,  976, 3032,  ..., 4686, 1200, 2217]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74125493",
   "metadata": {},
   "source": [
    "# 6 - Entrenamiento\n",
    "\n",
    "Ahora entrenaremos el modelo utilizando los datos de muestra. En la práctica, utilizaríamos un conjunto de datos mayor y lo dividiríamos en conjuntos de entrenamiento y validación.\n",
    "\n",
    "\n",
    "Podemos usar esta forma para construir un Transformer simple desde cero en Pytorch. Todos los grandes modelos lingüísticos utilizan estos bloques Transformer codificadores o decodificadores para el entrenamiento. Por lo tanto, entender la red que lo empezó todo es extremadamente importante. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d122b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, \n",
    "                          num_layers, d_ff, max_seq_length, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd2d26e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a4ca5dc5a034713a42f417877b50d06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 8.688794136047363\n",
      "Epoch: 2, Loss: 8.55346965789795\n",
      "Epoch: 3, Loss: 8.48188591003418\n",
      "Epoch: 4, Loss: 8.424513816833496\n",
      "Epoch: 5, Loss: 8.364824295043945\n",
      "Epoch: 6, Loss: 8.294319152832031\n",
      "Epoch: 7, Loss: 8.209874153137207\n",
      "Epoch: 8, Loss: 8.130683898925781\n",
      "Epoch: 9, Loss: 8.047629356384277\n",
      "Epoch: 10, Loss: 7.961203098297119\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.train()\n",
    "\n",
    "for epoch in tqdm(range(10)):\n",
    "    optimizer.zero_grad()\n",
    "    output = transformer(src_data, tgt_data[:, :-1])\n",
    "    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a34800d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual",
   "language": "python",
   "name": "virtual"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
