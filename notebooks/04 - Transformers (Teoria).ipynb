{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28cd770d",
   "metadata": {},
   "source": [
    "# 04 - Transformers (Teoría)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4760cf2",
   "metadata": {},
   "source": [
    "Transformers nació con el paper Attention is All You Need. El grupo de NLP de Harvard creó una guía anotando el paper con la implementación en PyTorch. Vamos a intentar simplificar un poco las cosas e introducir los conceptos uno por uno para que sea más fácil de entender."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d12ddc",
   "metadata": {},
   "source": [
    "### Una mirada de alto nivel\n",
    "\n",
    "Empecemos por ver el modelo como una caja negra. En una aplicación de traducción automática, tomaría una frase en un idioma y la traduciría a otro."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476c9a57",
   "metadata": {},
   "source": [
    "![transformers_exp_1](../images/transformers_exp_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9330f801",
   "metadata": {},
   "source": [
    "En las tripas de Optimus Prime, hay un componente de codificación (encoder), un componente de descodificación (decoder) y conexiones entre ellos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb32a568",
   "metadata": {},
   "source": [
    "![transformers_exp_2](../images/transformers_exp_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cfac67",
   "metadata": {},
   "source": [
    "El encoder es una pila de codificadores (en el paper se apilan seis de ellos uno encima de otro; no hay nada mágico en el número seis, se puede experimentar con otras disposiciones). El decoder es una pila de descodificadores del mismo número."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014e646b",
   "metadata": {},
   "source": [
    "![transformers_exp_3](../images/transformers_exp_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405c918a",
   "metadata": {},
   "source": [
    "Todos los encoders tienen la misma estructura, aunque no comparten sus pesos internos. Cada uno se divide en dos subcapas:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa6a96e",
   "metadata": {},
   "source": [
    "![transformers_exp_4](../images/transformers_exp_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52903c45",
   "metadata": {},
   "source": [
    "Las entradas del encoder pasan primero por una capa de \"auto-atención\" (self-attention), que ayuda al encoder a mirar otras palabras de la frase de entrada mientras codifica una palabra concreta. Más adelante veremos con más detalle la auto-atención.\n",
    "\n",
    "Las salidas de la capa de auto-atención se envían a una red neuronal feed-forward. Exactamente la misma red feed-forward se aplica independientemente a cada posición.\n",
    "\n",
    "El decoder tiene estas dos capas, pero entre ellas hay una capa de atención que ayuda al decoder a centrarse en las partes relevantes de la frase de entrada, algo similar a lo que hace la atención en los modelos seq2seq."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e1d609",
   "metadata": {},
   "source": [
    "![transformers_exp_5](../images/transformers_exp_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafb8726",
   "metadata": {},
   "source": [
    "### Introduciendo los tensores\n",
    "\n",
    "Ahora que hemos visto los principales bloques del modelo, vamos a empezar a ver los distintos vectores/tensores y cómo fluyen entre estos componentes para convertir la entrada de un modelo entrenado en una salida.\n",
    "\n",
    "Como ocurre en las aplicaciones NLP en general, empezamos convirtiendo cada palabra de entrada en un vector mediante un algoritmo de incrustado (word embeddings)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73758bec",
   "metadata": {},
   "source": [
    "![transformers_exp_6](../images/transformers_exp_6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3567bffd",
   "metadata": {},
   "source": [
    "Cada palabra está incrustada en un vector de tamaño 512. Representaremos esos vectores con estos simples recuadros.\n",
    "\n",
    "\n",
    "El embebido sólo se produce en el primer encoder, el situado más abajo. La abstracción que es común a todos los codificadores es que reciben una lista de vectores cada uno del tamaño 512. En el encoder de la siguiente imagen eso serían las palabras embebidas, pero en otros codificadores, sería la salida del codificador que está directamente debajo. El tamaño de esta lista es un hiperparámetro que podemos establecer, sería básicamente la longitud de la frase más larga de nuestro conjunto de datos de entrenamiento.\n",
    "\n",
    "\n",
    "Después de embeber las palabras en nuestra secuencia de entrada, cada una de ellas fluye a través de cada una de las dos capas del codificador."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e17e441",
   "metadata": {},
   "source": [
    "![transformers_exp_7](../images/transformers_exp_7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44d1030",
   "metadata": {},
   "source": [
    "Aquí empezamos a ver una propiedad clave del Transformer, que es que la palabra en cada posición fluye a través de su propio camino en el codificador. Existen dependencias entre estos caminos en la capa de auto-atención. Sin embargo, la capa feed-forward no tiene esas dependencias y, por tanto, los distintos caminos pueden ejecutarse en paralelo mientras fluyen a través de la capa feed-forward.\n",
    "\n",
    "A continuación, cambiaremos el ejemplo a una frase más corta y veremos lo que ocurre en cada subcapa del encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6e5732",
   "metadata": {},
   "source": [
    "### Ahora encoder\n",
    "\n",
    "Como ya hemos dicho, un encoder recibe una lista de vectores como entrada. Procesa esta lista pasando estos vectores a una capa de \"auto-atención\", luego a una red neuronal feed-forward y, por último, envía la salida hacia arriba al siguiente encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fee069",
   "metadata": {},
   "source": [
    "![transformers_exp_8](../images/transformers_exp_8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61adac6b",
   "metadata": {},
   "source": [
    "La palabra en cada posición pasa por un proceso de auto-atención. A continuación, cada una de ellas pasa por una red neuronal feed-forward, exactamente la misma red, pero cada vector fluye por ella por separado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac39652",
   "metadata": {},
   "source": [
    "### Autoatención a alto nivel\n",
    "\n",
    "\n",
    "No nos engañemos con la palabra \"auto-atención\", como si fuera un concepto con el que todo el mundo debería estar familiarizado. Todo está explicado en el paper \"Attention is All You Need\". Veamos cómo funciona.\n",
    "\n",
    "\n",
    "Supongamos que queremos traducir la siguiente frase:\n",
    "\n",
    "\"The animal didn't cross the street because it was too tired\".\n",
    "\n",
    "¿A qué se refiere \"it\" en esta frase? ¿Se refiere a la calle o al animal? Es una pregunta sencilla para un humano, pero no tanto para un algoritmo.\n",
    "\n",
    "Cuando el modelo está procesando la palabra \"it\", la auto-atención le permite asociar \"it\" con \"animal\".\n",
    "\n",
    "A medida que el modelo procesa cada palabra, cada posición en la secuencia de entrada, la auto-atención le permite buscar pistas en otras posiciones de la secuencia de entrada que le ayuden a codificar mejor esta palabra.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9bb8ea",
   "metadata": {},
   "source": [
    "![transformers_exp_9](../images/transformers_exp_9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf7bc43",
   "metadata": {},
   "source": [
    "Como estamos codificando la palabra \"it\" en el encoder nº5, el encoder superior de la pila, parte del mecanismo de atención se estaba centrando en \"The animal\", e incorporó una parte de su representación a la codificación de \"it\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5543f7c",
   "metadata": {},
   "source": [
    "### Auto-atención en detalle\n",
    "\n",
    "\n",
    "Veamos primero cómo calcular la auto-atención utilizando vectores y, a continuación, cómo se implementa realmente, utilizando matrices.\n",
    "\n",
    "**El primer paso** para calcular la auto-atención es crear tres vectores a partir de cada uno de los vectores de entrada del encoder, en este caso, el embebido de cada palabra. Así, para cada palabra, creamos un vector Query, un vector Key y un vector Value. Estos vectores se crean multiplicando el embebido por tres matrices que entrenamos durante el proceso de formación.\n",
    "\n",
    "Estos nuevos vectores tienen una dimensión menor que el vector embebido. Su dimensionalidad es de 64, mientras que los vectores de entrada/salida del embebido y del encoder tienen una dimensionalidad de 512. No TIENEN que ser más pequeños, esta es una elección de arquitectura para hacer el cómputo de la multi-atención (multiheaded attention) en su mayoría constante."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9b9b31",
   "metadata": {},
   "source": [
    "![transformers_exp_10](../images/transformers_exp_10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022de869",
   "metadata": {},
   "source": [
    "Al multiplicar $x1$ por la matriz de pesos $WQ$ se obtiene $q1$, el vector de Query asociado a esa palabra. Al final creamos una proyección de Query, una de Key y una de Value de cada palabra de la frase de entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e565338",
   "metadata": {},
   "source": [
    "¿Qué son los vectores Query, Key y Value?\n",
    "\n",
    "Son abstracciones útiles para calcular y pensar sobre la atención. Veamos como se calcula la atención para entenderlo.\n",
    "\n",
    "**El segundo paso** para calcular la auto-atención es calcular una puntuación. Digamos que estamos calculando la auto-atención para la primera palabra de este ejemplo, \"Thinking\". Tenemos que puntuar cada palabra de la frase de entrada con respecto a esta palabra. La puntuación determina el grado de atención que debemos prestar a otras partes de la frase de entrada cuando codificamos una palabra en una posición determinada.\n",
    "\n",
    "La puntuación se calcula tomando el producto escalar del vector Query por el vector Key de la palabra correspondiente. Así, si estamos procesando la auto-atención de la palabra en la posición nº 1, la primera puntuación sería el producto punto de $q1$ y $k1$. La segunda puntuación sería el producto escalar de $q1$ y $k2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133ae66c",
   "metadata": {},
   "source": [
    "![transformers_exp_11](../images/transformers_exp_11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6182784",
   "metadata": {},
   "source": [
    "**El tercer y cuarto paso** consisten en dividir las puntuaciones por 8, la raíz cuadrada de la dimensión de los vectores Key utilizados en el paper: 64. Esto nos lleva a tener gradientes más estables. Podría haber otros valores posibles aquí, pero este es el predeterminado. Luego pasar el resultado a través de una operación softmax. Softmax normaliza las puntuaciones para que sean todas positivas y sumen 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9a5cd0",
   "metadata": {},
   "source": [
    "![transformers_exp_12](../images/transformers_exp_12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9185dc99",
   "metadata": {},
   "source": [
    "Esta puntuación softmax determina cuánto se expresará cada palabra en esta posición. Está claro que la palabra en esta posición tendrá la puntuación softmax más alta, pero a veces es útil atender a otra palabra que sea relevante para la palabra actual."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c888038",
   "metadata": {},
   "source": [
    "**El quinto paso** consiste en multiplicar cada vector Value por la puntuación softmax, para luego sumarlos. La intuición aquí es mantener intactos los valores de la(s) palabra(s) en las que queremos centrarnos, y ahogar las palabras irrelevantes, multiplicándolas por números minúsculos como 0,001, por ejemplo.\n",
    "\n",
    "\n",
    "**El sexto paso** consiste en sumar los vectores de valores ponderados. Esto produce la salida de la capa de auto-atención en esta posición, para la primera palabra."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e35d69",
   "metadata": {},
   "source": [
    "![transformers_exp_13](../images/transformers_exp_13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c25ccc9",
   "metadata": {},
   "source": [
    "Con esto concluye el cálculo de la auto-atención. El vector resultante es el que podemos enviar a la red neuronal feed-forward. En la implementación real, sin embargo, este cálculo se realiza en forma de matriz para un procesamiento más rápido. Veámoslo ahora que hemos visto la intuición del cálculo a nivel de palabra."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72272f5",
   "metadata": {},
   "source": [
    "## Cálculo de la matriz de autoatención\n",
    "\n",
    "\n",
    "**El primer paso** consiste en calcular las matrices de Query, Key y Value. Para ello, empaquetamos nuestros embebidos en una matriz X y la multiplicamos por las matrices de pesos que hemos entrenado (WQ, WK, WV)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef94fb87",
   "metadata": {},
   "source": [
    "![transformers_exp_14](../images/transformers_exp_14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49692b4a",
   "metadata": {},
   "source": [
    "Cada fila de la matriz X corresponde a una palabra de la frase de entrada. Volvemos a ver la diferencia de tamaño entre el vector embebido, 512, o 4 casillas en la figura, y los vectores $q$/$k$/$v$, 64, o 3 casillas en la figura.\n",
    "\n",
    "\n",
    "**Por último**, como se trata de matrices, podemos condensar los **pasos dos a seis en una fórmula** para calcular los resultados de la capa de auto-atención."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7ae82c",
   "metadata": {},
   "source": [
    "![transformers_exp_15](../images/transformers_exp_15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1780bc54",
   "metadata": {},
   "source": [
    "## La bestia de muchas cabezas\n",
    "\n",
    "El paper perfecciona la capa de auto-atención añadiendo un mecanismo denominado multi-atención (multi-headed attention). Esto mejora el rendimiento de la capa de atención de dos maneras:\n",
    "\n",
    "1. Amplía la capacidad del modelo para centrarse en distintas posiciones. Sí, en el ejemplo anterior, $z1$ contiene un poco de cualquier otra codificación, pero podría estar dominada por la propia palabra. Si estamos traduciendo una frase como \"The animal didn’t cross the street because it was too tired\", sería útil saber a qué palabra se refiere \"it\".\n",
    "\n",
    "2. Esto proporciona a la capa de atención múltiples \"subespacios de representación\". Como veremos a continuación, con la multi-atención tenemos no sólo uno, sino múltiples conjuntos de matrices de pesos Query/Key/Value, el Transformer utiliza ocho cabezales de atención, por lo que acabamos con ocho conjuntos para cada encoder/decoder. Cada uno de estos conjuntos se inicializa aleatoriamente. Después del entrenamiento, cada conjunto se utiliza para proyectar las incrustaciones de entrada, o vectores de codificadores/decodificadores inferiores, en un subespacio de representación diferente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329bdd25",
   "metadata": {},
   "source": [
    "![transformers_exp_16](../images/transformers_exp_16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b419be8c",
   "metadata": {},
   "source": [
    "Con multi-atención, mantenemos las matrices de pesos Q/K/V separadas para cada cabezal, lo que da lugar a matrices Q/K/V diferentes. Como hicimos antes, multiplicamos X por las matrices WQ/WK/WV para producir matrices Q/K/V.\n",
    "\n",
    "Si hacemos el mismo cálculo de auto-atención que esbozamos antes, sólo que ocho veces diferentes con matrices de peso distintas, acabamos con ocho matrices Z diferentes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0807d99",
   "metadata": {},
   "source": [
    "![transformers_exp_17](../images/transformers_exp_17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce85c494",
   "metadata": {},
   "source": [
    "Esto nos plantea un pequeño reto. La capa feed-forward no espera ocho matrices, sino una sola, un vector por cada palabra. Así que necesitamos una forma de condensar esas ocho matrices en una sola.\n",
    "\n",
    "¿Cómo lo hacemos? Concatenamos las matrices y las multiplicamos por una matriz de pesos adicional WO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7960972",
   "metadata": {},
   "source": [
    "![transformers_exp_18](../images/transformers_exp_18.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12a5ffa",
   "metadata": {},
   "source": [
    "Eso es más o menos todo lo que hay en la auto-atención multicabeza. Es un buen puñado de matrices. Vamos a ponerlos todos en una gráfica para que podamos verlos en un solo lugar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c75b9d4",
   "metadata": {},
   "source": [
    "![transformers_exp_19](../images/transformers_exp_19.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b4012c",
   "metadata": {},
   "source": [
    "Ahora que hemos hablado de las cabezas de atención, volvamos a nuestro ejemplo anterior para ver dónde se centran las diferentes cabezas de atención cuando codificamos la palabra \"it\" en nuestra frase de ejemplo:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625f5efc",
   "metadata": {},
   "source": [
    "![transformers_exp_20](../images/transformers_exp_20.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ff163b",
   "metadata": {},
   "source": [
    "Cuando codificamos la palabra \"it\", una cabeza de atención se centra sobre todo en \"The animal\", mientras que otra se centra en \"tired\"; en cierto sentido, la representación del modelo de la palabra \"it\" incorpora parte de la representación tanto de \"animal\" como de \"tired\".\n",
    "\n",
    "\n",
    "Sin embargo, si añadimos todas las cabezas de atención a la imagen, las cosas pueden ser más difíciles de interpretar:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad280fb7",
   "metadata": {},
   "source": [
    "![transformers_exp_21](../images/transformers_exp_21.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed8b7a6",
   "metadata": {},
   "source": [
    "# Representación del orden de la secuencia mediante codificación posicional\n",
    "\n",
    "Lo que falta en el modelo tal y como lo hemos descrito hasta ahora es una forma de tener en cuenta el orden de las palabras en la secuencia de entrada.\n",
    "\n",
    "Para ello, el transformer añade un vector a cada embebido de entrada. Estos vectores siguen un patrón específico que el modelo aprende y que le ayuda a determinar la posición de cada palabra o la distancia entre las distintas palabras de la secuencia. La intuición aquí es que añadir estos valores a los embebidos proporciona distancias significativas entre los vectores una vez que se proyectan en vectores Q/K/V y durante la atención en el producto escalar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e1f93d",
   "metadata": {},
   "source": [
    "![transformers_exp_22](../images/transformers_exp_22.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cfe7be",
   "metadata": {},
   "source": [
    "Para dar al modelo una idea del orden de las palabras, añadimos vectores de codificación posicional, cuyos valores siguen un patrón específico.\n",
    "\n",
    "\n",
    "Si suponemos que el embebido tiene una dimensionalidad de 4, las codificaciones posicionales reales tendrían este aspecto:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d3cdc5",
   "metadata": {},
   "source": [
    "![transformers_exp_23](../images/transformers_exp_23.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b83b2b0",
   "metadata": {},
   "source": [
    "¿Qué aspecto podría tener este patrón?\n",
    "\n",
    "En la siguiente figura, cada fila corresponde a una codificación posicional de un vector. Así, la primera fila sería el vector que añadiríamos al embebido de la primera palabra de una secuencia de entrada. Cada fila contiene 512 valores, cada uno con un valor entre 1 y -1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dde1aee",
   "metadata": {},
   "source": [
    "![transformers_exp_24](../images/transformers_exp_24.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d053937",
   "metadata": {},
   "source": [
    "Este es un ejemplo real de codificación posicional para 20 palabras (filas) con un tamaño de embebido de 512 (columnas). Se puede ver que aparece partido por la mitad en el centro. Esto se debe a que los valores de la mitad izquierda son generados por una función (que utiliza el seno), y la mitad derecha es generada por otra función (que utiliza el coseno). Luego se concatenan para formar cada uno de los vectores de codificación posicional.\n",
    "La fórmula de la codificación posicional se describe en el paper (sección 3.5). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e53213",
   "metadata": {},
   "source": [
    "# Los residuos\n",
    "\n",
    "Un detalle en la arquitectura del codificador que debemos mencionar antes de seguir adelante, es que cada subcapa (auto-atención, feed-forward) en cada encoder tiene una conexión residual a su alrededor, y es seguida por un paso de normalización de capas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d597713",
   "metadata": {},
   "source": [
    "![transformers_exp_25](../images/transformers_exp_25.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4527e539",
   "metadata": {},
   "source": [
    "Si visualizáramos los vectores y la operación aññadir-normalizar asociada a la auto-atención, se vería así:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b39549",
   "metadata": {},
   "source": [
    "![transformers_exp_26](../images/transformers_exp_26.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34db1901",
   "metadata": {},
   "source": [
    "Lo mismo ocurre con las subcapas del decoder. Si pensamos en un Transformer de 2 encoders y decoders apilados, sería algo así:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1a09d8",
   "metadata": {},
   "source": [
    "![transformers_exp_27](../images/transformers_exp_27.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157a3548",
   "metadata": {},
   "source": [
    "# El lado del decoder\n",
    "\n",
    "Ahora que hemos estudiado la mayoría de los conceptos del lado del encoder, ya sabemos cómo funcionan los componentes de los decoders. Pero veamos cómo funcionan juntos.\n",
    "\n",
    "El encoder comienza procesando la secuencia de entrada. A continuación, la salida del encoder superior se transforma en un conjunto de vectores de atención K y V. Cada decoder los utiliza en su capa de \"atención encoder-decoder\", que ayuda al decoder a centrarse en los lugares adecuados de la secuencia de entrada:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a227fb9c",
   "metadata": {},
   "source": [
    "![transformers_exp_28](../images/transformers_exp_28.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af38820f",
   "metadata": {},
   "source": [
    "Una vez finalizada la fase de codificación, comenzamos la fase de descodificación. Cada paso de la fase de descodificación da salida a un elemento de la secuencia de salida, la frase traducida al inglés en este caso.\n",
    "\n",
    "Los pasos siguientes repiten el proceso hasta que se alcanza un símbolo especial que indica que el decoder del transformer ha completado su salida. La salida de cada paso se envía al decoder inferior en el siguiente paso temporal, y los decoders burbujean sus resultados de descodificación igual que hicieron los encoders. Y al igual que hicimos con las entradas del encoder, embebemos y añadimos codificación posicional a esas entradas del decoder para indicar la posición de cada palabra."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47e9e4b",
   "metadata": {},
   "source": [
    "![transformers_exp_29](../images/transformers_exp_29.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545e3844",
   "metadata": {},
   "source": [
    "Las capas de auto-atención del decoder funcionan de forma ligeramente distinta a las del encoder:\n",
    "\n",
    "En el decoder, a la capa de auto-atención sólo se le permite atender a posiciones anteriores en la secuencia de salida. Esto se hace enmascarando las posiciones futuras, poniéndolas a -inf, antes del paso softmax en el cálculo de auto-atención.\n",
    "\n",
    "La capa \"Atención Encoder-Decoder\" funciona igual que la multi-atención, excepto que crea su matriz Query a partir de la capa inferior, y toma la matriz Key y Value de la salida de la pila del encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9438d840",
   "metadata": {},
   "source": [
    "# Capa final lineal y Softmax\n",
    "\n",
    "La pila del decoder produce un vector de floats. ¿Cómo lo convertimos en una palabra? De eso se encarga la última capa lineal, a la que sigue una capa Softmax.\n",
    "\n",
    "La capa lineal es una sencilla red neuronal totalmente conectada que proyecta el vector producido por la pila de decoders en un vector mucho mayor, llamado vector logit.\n",
    "\n",
    "Supongamos que nuestro modelo conoce 10.000 palabras únicas en inglés, el \"vocabulario de salida\" de nuestro modelo, que ha aprendido de su conjunto de datos de entrenamiento. Esto haría que el vector logit tuviera 10.000 celdas de ancho, cada una de las cuales correspondería a la puntuación de una palabra única. Así es como interpretamos la salida del modelo seguido por la capa Lineal.\n",
    "\n",
    "A continuación, la capa softmax convierte esas puntuaciones en probabilidades, todas positivas, todas suman 1. Se elige la celda con la probabilidad más alta, y la palabra asociada a ella se produce como la salida para este paso de tiempo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ef5121",
   "metadata": {},
   "source": [
    "![transformers_exp_30](../images/transformers_exp_30.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed87fd0",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# Entrenamiento\n",
    "\n",
    "Ahora que hemos cubierto todo el proceso de paso adelante a través de un Transformer entrenado, sería útil echar un vistazo al propio entrenamiento del modelo.\n",
    "\n",
    "Durante el entrenamiento, un modelo no entrenado pasaría exactamente por el mismo proceso. Pero como lo estamos entrenando en un conjunto de datos de entrenamiento etiquetados, podemos comparar su resultado con el resultado correcto real.\n",
    "\n",
    "Para visualizarlo, supongamos que nuestro vocabulario de salida sólo contiene seis palabras (\"a\", \"am\", \"i\", \"thanks\", \"student\" y \"<eos>\" (abreviatura de \"end of sentence\"))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013bd922",
   "metadata": {},
   "source": [
    "![transformers_exp_31](../images/transformers_exp_31.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89b132f",
   "metadata": {},
   "source": [
    "El vocabulario de salida de nuestro modelo se crea en la fase de preprocesamiento, antes incluso de empezar el entrenamiento.\n",
    "\n",
    "Una vez definido nuestro vocabulario de salida, podemos utilizar un vector de la misma anchura para indicar cada palabra de nuestro vocabulario. Esto también se conoce como codificación one-hot. Así, por ejemplo, podemos indicar la palabra \"am\" utilizando el siguiente vector:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ed5327",
   "metadata": {},
   "source": [
    "![transformers_exp_32](../images/transformers_exp_32.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b668eb04",
   "metadata": {},
   "source": [
    "Tras este resumen, hablemos de la función de pérdida del modelo, la métrica que optimizamos durante la fase de entrenamiento para llegar a un modelo entrenado y, con suerte, asombrosamente preciso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d21e2c2",
   "metadata": {},
   "source": [
    "# La función de pérdida\n",
    "\n",
    "Supongamos que estamos entrenando nuestro modelo. Digamos que es nuestro primer paso en la fase de entrenamiento y que lo estamos entrenando con un ejemplo sencillo: traducir \"merci\" por \"thanks\".\n",
    "\n",
    "Esto significa que queremos que la salida sea una distribución de probabilidad que indique la palabra \"thanks\". Pero como este modelo aún no está entrenado, es poco probable que eso ocurra todavía."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef32261",
   "metadata": {},
   "source": [
    "![transformers_exp_33](../images/transformers_exp_33.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a695520",
   "metadata": {},
   "source": [
    "Dado que los parámetros del modelo, pesos, se inicializan aleatoriamente, el modelo no entrenado produce una distribución de probabilidad con valores arbitrarios para cada celda/palabra. Podemos compararla con la salida real y, a continuación, ajustar todos los pesos del modelo mediante retropropagación para que la salida se acerque más a la salida deseada.\n",
    "\n",
    "\n",
    "¿Cómo se comparan dos distribuciones de probabilidad? Simplemente restamos una de la otra. Para más detalles, consulte la entropía cruzada y la divergencia de Kullback-Leibler.\n",
    "\n",
    "Pero tenga en cuenta que se trata de un ejemplo demasiado simplificado. Para ser más realistas, utilizaremos una frase de más de una palabra. Por ejemplo: \"je suis étudiant\" y salida esperada: \"i am a student\". Lo que esto significa realmente es que queremos que nuestro modelo produzca sucesivamente distribuciones de probabilidad donde:\n",
    "\n",
    "+ Cada distribución de probabilidad está representada por un vector de anchura vocab_size, 6 en nuestro ejemplo de juguete, pero más realista un número como 30.000 o 50.000.\n",
    "+ La primera distribución de probabilidad tiene la probabilidad más alta en la celda asociada a la palabra \"i\".\n",
    "+ La segunda distribución de probabilidad tiene la mayor probabilidad en la celda asociada a la palabra \"am\".\n",
    "+ Y así sucesivamente, hasta que la quinta distribución de salida indica el símbolo \"<eos>\", que también tiene asociada una celda del vocabulario de 10.000 elementos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29445bdd",
   "metadata": {},
   "source": [
    "![transformers_exp_34](../images/transformers_exp_34.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c455b70",
   "metadata": {},
   "source": [
    "Las distribuciones de probabilidad objetivo con las que entrenaremos nuestro modelo en el ejemplo de entrenamiento para una frase de muestra.\n",
    "\n",
    "\n",
    "Después de entrenar el modelo durante el tiempo suficiente en un conjunto de datos suficientemente grande, esperamos que las distribuciones de probabilidad producidas tengan este aspecto:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7df39d",
   "metadata": {},
   "source": [
    "![transformers_exp_35](../images/transformers_exp_35.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416b7abf",
   "metadata": {},
   "source": [
    "Es de esperar que, tras el entrenamiento, el modelo produzca la traducción correcta que esperamos. Por supuesto, no hay ninguna indicación real de si esta frase formaba parte del conjunto de datos de entrenamiento, véase validación cruzada. Cada posición recibe un poco de probabilidad, incluso si es poco probable que sea la salida de ese paso de tiempo, que es una propiedad muy útil de softmax que ayuda al proceso de formación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7853efeb",
   "metadata": {},
   "source": [
    "Ahora, como el modelo produce las salidas de una en una, podemos suponer que el modelo está seleccionando la palabra con la probabilidad más alta de esa distribución de probabilidad y desechando el resto. Esa es una forma de hacerlo, llamada decodificación codiciosa (greedy decoding). Otra forma de hacerlo sería quedarnos, por ejemplo, con las dos primeras palabras, por ejemplo, \"I\" y \"a\", y, en el siguiente paso, ejecutar el modelo dos veces: una vez suponiendo que la primera posición de salida es la palabra \"I\" y otra vez suponiendo que la primera posición de salida es la palabra \"a\". Repetimos esto para las posiciones 2, 3...etc. Este método se llama \"búsqueda de haces\" (beam search), donde en nuestro ejemplo, beam_size era dos, lo que significa que en todo momento se guardan en memoria dos hipótesis parciales, traducciones inacabadas, y top_beams es también dos, lo que significa que devolveremos dos traducciones. Ambos son hiperparámetros con los que se puede experimentar."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual",
   "language": "python",
   "name": "virtual"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
