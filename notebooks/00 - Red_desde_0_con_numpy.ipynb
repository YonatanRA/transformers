{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35f53766",
   "metadata": {},
   "source": [
    "# 00 - Red Neuronal desde 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93e2549",
   "metadata": {},
   "source": [
    "Crearemos una red desde cero con numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e082cf8",
   "metadata": {},
   "source": [
    "**Proceso de funcionamiento:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476c3bbb",
   "metadata": {},
   "source": [
    "+ **1** - Le pasamos los datos a la red.\n",
    "\n",
    "Todas las capas que creemos tienen al menos 2 cosas en común: una entrada y una salida de datos.\n",
    "\n",
    "<br>\n",
    "\n",
    "![forward_1](../images/forward_1.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbb5803",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "+ **2** - Los datos se transforman capa tras capa hasta llegar a la salida de la red.\n",
    "\n",
    "Hay que remarcar que la salida de una capa es la entrada de la siguiente capa. A este proceso se le conoce como propagación hacia adelante (forward propagation).\n",
    "\n",
    "<br>\n",
    "\n",
    "+ **3** - En la salida de la red se calcula el error, un número, comparando la predicción con la verdad (ground truth). Recordemos que es aprendizaje supervisado.\n",
    "\n",
    "<br>\n",
    "\n",
    "![forward_2](../images/forward_2.webp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98705c6b",
   "metadata": {},
   "source": [
    "+ **4** - Ajustamos los pesos, o el sesgo, restando la derivada parcial del error con respecto al propio peso.\n",
    "\n",
    "<br>\n",
    "\n",
    "![gradient_desc](../images/gradient_desc.webp)\n",
    "\n",
    "<br>\n",
    "\n",
    "Este proceso es conocido como gradiente descendente, básicamente queremos cambiar el valor del peso $\\omega$ con el objetivo de minimizar el error de la red con respecto de la verdad. El parámetro $\\alpha$ es un parámetro de ajuste llamado tasa de aprendizaje comprendido en el intervalo [0, 1].\n",
    "\n",
    "Tenemos que repetir esto capa a capa. Supongamos que le damos a una capa la derivada del error con respecto a su salida ($∂E/∂Y$), entonces la capa debe ser capaz de calcular la derivada del error con respecto a su entrada ($∂E/∂X$). A este proceso se le conoce como propagación hacia atrás (backward propagation - backpropagation).\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "![backward_prop](../images/backward_prop.webp)\n",
    "\n",
    "<br>\n",
    "\n",
    "Recordemos que el error es un número calculado al final de la red, y que tanto $X$ como $Y$ son matrices.\n",
    "\n",
    "<br>\n",
    "\n",
    "![backward_prop2](../images/backward_prop2.webp)\n",
    "\n",
    "<br>\n",
    "\n",
    "El truco es que teniendo acceso a $∂E/∂Y$ podemos calcular fácilmente $∂E/∂\\omega$, si la capa tiene parámetros entrenables, sin conocer siquiera la estructura de la red. Es tan simple como usar la regla de la cadena:\n",
    "\n",
    "<br>\n",
    "\n",
    "![backward_prop3](../images/backward_prop3.webp)\n",
    "\n",
    "<br>\n",
    "\n",
    "La incógnita es $∂y_{j}/∂w$, que depende de como la capa calcula su salida. Asi que si cada capa acceso a $∂E/∂Y$,\n",
    "donde $Y$ es su propia salida, entonces podemos actualizar nuestros parámetros. Sin olvidar que la salida de una capa es la entrada de la siguiente, lo que quiere decir que $∂E/∂X$ para una capa es $∂E/∂Y$ para la anterior. Es asi como se propaga el error hacia atrás. De nuevo, por la regla de la cadena:\n",
    "\n",
    "<br>\n",
    "\n",
    "![backward_prop4](../images/backward_prop4.webp)\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "**Diagrama resumen**\n",
    "\n",
    "<br>\n",
    "\n",
    "![backward_prop5](../images/backward_prop5.webp)\n",
    "\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab0a87c",
   "metadata": {},
   "source": [
    "+ **5** - Iteramos el proceso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0eafc9",
   "metadata": {},
   "source": [
    "## Código de Python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e21845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf34363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero la clase básica de lo que es una capa, el molde\n",
    "\n",
    "class Layer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Todas las capa tienes entrada y salida de datos, método constructor\n",
    "        '''\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "        \n",
    "    def forward_propagation(self, input_):\n",
    "        '''\n",
    "        Propagación hacia adelante, calcula la salida Y de la capa dada la entrada X\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        '''\n",
    "        Propagación hacia atrás, calcula dE/dX dado dE/dY, y actuliza parámetros \n",
    "        '''\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d871abdf",
   "metadata": {},
   "source": [
    "### Capa Dense (Full Connected)\n",
    "\n",
    "Desde código vamos a crear una capa completamente conectada, que recuerda y mucho a una regresión lineal.\n",
    "\n",
    "<br>\n",
    "\n",
    "![dense](../images/dense.webp)\n",
    "\n",
    "<br>\n",
    "\n",
    "De hecho el forward se reduce a la siguiente ecuación:\n",
    "\n",
    "<br>\n",
    "\n",
    "![dense2](../images/dense2.webp)\n",
    "\n",
    "<br>\n",
    "\n",
    "y el backward a las tres siguientes:\n",
    "\n",
    "<br>\n",
    "\n",
    "![dense3](../images/dense3.webp)\n",
    "\n",
    "<br>\n",
    "\n",
    "Lo cuál es el resumen de la minimización por mínimos cuadrados, pues recordemos que nuestra función de pérdida es:\n",
    "\n",
    "<br>\n",
    "\n",
    "![dense4](../images/dense4.webp)\n",
    "\n",
    "<br>\n",
    "\n",
    "![dense5](../images/dense5.webp)\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92695784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# capa completamente conectada\n",
    "\n",
    "# hereda de la clase base Layer\n",
    "class Dense(Layer):\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        '''\n",
    "        input_size = número de neuronas de entrada\n",
    "        output_size = número de neuronas de salida\n",
    "        '''\n",
    "        \n",
    "        # inicialización aleatoria de parámetros con las dimensiones adecuadas\n",
    "        self.weights = np.random.rand(input_size, output_size) - 0.5   # pesos, betas 1-n (W)\n",
    "        self.bias = np.random.rand(1, output_size) - 0.5               # sesgo, ordenada en el origen, beta0 (B)\n",
    "\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        '''\n",
    "        Dados unos datos de entrada X, devuelve Y = XW+B\n",
    "        '''\n",
    "        \n",
    "        self.input = input_data   # datos de entrada, atributo heredado\n",
    "        \n",
    "        self.output = np.dot(self.input, self.weights) + self.bias  # salida Y = XW+B, atributo heredado\n",
    "        \n",
    "        return self.output\n",
    "    \n",
    "\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        '''\n",
    "        Dados el error de salida (output_error=dE/dY) y la tasa de aprendizaje,\n",
    "        calcula dE/dX, dE/dW y dE/dB, y devuelve el error de entrada (input_error=dE/dX)\n",
    "        '''\n",
    "        \n",
    "        input_error = np.dot(output_error, self.weights.T)  # dE/dX = dE/dY * W^t\n",
    "        weights_error = np.dot(self.input.T, output_error)  # dE/dW = X^t * dE/dY\n",
    "        # dBias = output_error                              # dE/dB = dE/dY\n",
    "\n",
    "        # update parameters\n",
    "        self.weights -= learning_rate * weights_error    # W = W - a*dE/dW\n",
    "        self.bias -= learning_rate * output_error        # B = B - a*dE/dY\n",
    "        \n",
    "        return input_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60edd0f",
   "metadata": {},
   "source": [
    "### Capa de Activación\n",
    "\n",
    "Todos los calculos realizados hasta ahora son lineales. Debemos añadir no linealidad si queremos que el modelo aprenda mejor y pueda ser utilizado para más propósitos. Tan solo tenemos que añadir la capa de activación con la función de activación deseada y evaluar la función y su derivada:\n",
    "\n",
    "<br>\n",
    "\n",
    "![acti1](../images/acti1.webp)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "![acti2](../images/acti2.webp)\n",
    "\n",
    "<br>\n",
    "\n",
    "![acti3](../images/acti3.webp)\n",
    "\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f565d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# capa de activación base, valida para cualquier función de activación\n",
    "\n",
    "# hereda de la clase base Layer\n",
    "class Activation(Layer):\n",
    "    \n",
    "    def __init__(self, activation, activation_prime):\n",
    "        '''\n",
    "        activation = función de activación\n",
    "        activation_prime = derivada de la función de activación\n",
    "        '''\n",
    "        \n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        '''\n",
    "        Devuelve la evaluación de la función de activación\n",
    "        '''\n",
    "        \n",
    "        self.input = input_data\n",
    "        self.output = self.activation(self.input)\n",
    "        \n",
    "        return self.output\n",
    "\n",
    "    \n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        '''\n",
    "        Dado el error de salida (output_error=dE/dY),\n",
    "        calcula dE/dX, en este caso no se usa la tasa de aprendizaje,\n",
    "        no hay nada que aprender, solo evaluar.\n",
    "        '''\n",
    "        \n",
    "        return self.activation_prime(self.input) * output_error  # dE/dX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4fa40f",
   "metadata": {},
   "source": [
    "**Ejemplo función de activación**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c6fa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tangente hiperbólica\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "\n",
    "# derivada de la tangente hiperbólica\n",
    "def tanh_prime(x):\n",
    "    return 1 - np.tanh(x)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8d1bd8",
   "metadata": {},
   "source": [
    "### Función de Pérdida/Coste\n",
    "\n",
    "Objetivo de la minimización.\n",
    "\n",
    "<br>\n",
    "\n",
    "![dense4](../images/dense4.webp)\n",
    "\n",
    "<br>\n",
    "\n",
    "![dense5](../images/dense5.webp)\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84af15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# función de coste\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean(np.power(y_true-y_pred, 2))\n",
    "\n",
    "\n",
    "\n",
    "# derivada de la función de coste\n",
    "def mse_prime(y_true, y_pred):\n",
    "    return 2*(y_pred-y_true)/y_true.size;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5516b3b",
   "metadata": {},
   "source": [
    "### Clase Red Neuronal\n",
    "\n",
    "\n",
    "Ya prácticamente tenemos todo el código necesario para construir nuestra red neuronal. El código de la clase Network nos va a permitir añadir capas, usar distintas funciones de coste, entrenar nuestra red y también realizar predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93468e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clase red neuronal\n",
    "\n",
    "class Network:\n",
    "    \n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Se inician los atributos: capas y función de perdida.\n",
    "        '''\n",
    "        \n",
    "        self.layers = []          # lista de capas de la red\n",
    "        self.loss = None          # función de coste\n",
    "        self.loss_prime = None    # derivada de la función de coste\n",
    "\n",
    "        \n",
    "    \n",
    "    def add(self, layer):\n",
    "        '''\n",
    "        Método para añadir una capa a la red.\n",
    "        '''\n",
    "        self.layers.append(layer)\n",
    "\n",
    "        \n",
    "\n",
    "    def use(self, loss, loss_prime):\n",
    "        '''\n",
    "        Método para establecer la función de coste. Sobreescribe atributos\n",
    "        '''\n",
    "        self.loss = loss\n",
    "        self.loss_prime = loss_prime\n",
    "\n",
    "        \n",
    "\n",
    "    def predict(self, input_data):\n",
    "        '''\n",
    "        Método para predecir dada una entrada. Evalúa todas las capas.\n",
    "        Devuelve una lista de predicciones.\n",
    "        '''\n",
    "        \n",
    "        # número de muestras y lista vacía para guardar resultados\n",
    "        samples = len(input_data)\n",
    "        result = []\n",
    "\n",
    "        # para cada muestra, ejecuta la red..\n",
    "        for i in range(samples):\n",
    "            \n",
    "            # forward propagation, se evalúa cada capa\n",
    "            output = input_data[i]\n",
    "            \n",
    "            # para cada capa...\n",
    "            for layer in self.layers:\n",
    "                output = layer.forward_propagation(output)\n",
    "            \n",
    "            # guarda la prediccion\n",
    "            result.append(output)\n",
    "\n",
    "        return result\n",
    "\n",
    "    \n",
    "\n",
    "    def fit(self, X_train, y_train, epochs, learning_rate):\n",
    "        '''\n",
    "        Método para entrenar la red neuronal.\n",
    "        Recibe los datos de entrenamiento (X, y), el número de épocas y la tasa de apredizaje.\n",
    "        No devuelve nada, actualiza los pesos de cada capa de la red.\n",
    "        '''\n",
    "        \n",
    "        # número de muestras\n",
    "        samples = len(X_train)\n",
    "        \n",
    "        print_err = 0   # inicia error a cero, solo para ver\n",
    "        \n",
    "        # bucle de entrenamiento, para cada época....\n",
    "        for i in tqdm(range(epochs), desc='Epochs'):\n",
    "            \n",
    "            # para cada muestra de los datos...\n",
    "            for j in tqdm(range(samples), leave=False, bar_format=f'Loss Value --- {print_err}'):  \n",
    "                                \n",
    "                # forward propagation, se evalúa cada capa\n",
    "                output = X_train[j]\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward_propagation(output)\n",
    "                    \n",
    "                \n",
    "                # backward propagation, se actualizan los pesos de la red llevando el error hacia atrás \n",
    "                error = self.loss_prime(y_train[j], output)\n",
    "                for layer in reversed(self.layers):\n",
    "                    error = layer.backward_propagation(error, learning_rate)\n",
    "                    \n",
    "                \n",
    "                # calcula la pérdida, solo para ver el error\n",
    "                print_err = self.loss(y_train[j], output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00cd242",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274734a6",
   "metadata": {},
   "source": [
    "### Datos Mnist\n",
    "\n",
    "![mnist](../images/mnist.jpeg)\n",
    "\n",
    "El MNIST es un conjunto de datos desarrollado por Yann LeCun, Corinna Cortes y Christopher Burges para la evaluación de modelos de aprendizaje de máquinas sobre el problema de la clasificación de los dígitos escritos a mano. La base de datos se construyó a partir de varios conjuntos de datos de documentos escaneados disponibles en la carpeta Instituto Nacional de Estándares y Tecnología (NIST).\n",
    "\n",
    "Las imágenes de los dígitos fueron tomadas de una variedad de documentos escaneados, normalizados en tamaño y centrado. Esto lo convierte en un excelente conjunto de datos para evaluar modelos, permitiendo al desarrollador centrar el aprendizaje de la máquina con muy poca limpieza de datos. Cada imagen tiene 28 x 28 píxeles cuadrados (784 píxeles en total). Se utiliza una división estándar del conjunto de datos para evaluar y comparar modelos, en la que se utilizan 60.000 imágenes para formar un modelo y un conjunto separado de 10.000 imágenes para probarlo.\n",
    "\n",
    "Es una tarea de reconocimiento de dígitos. Como tal, hay 10 dígitos (0 a 9) o 10 clases para predecir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47817d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c327ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargar datos \n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff2f163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizar X_train\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28*28)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "\n",
    "X_train /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02d54c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833e8dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Antes:   ', y_train.shape)\n",
    "\n",
    "# one-hot para la y, es decir desde intervalo [0,9] a vector de tamaño 10\n",
    "\n",
    "# e.g. número 3 = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "y_train_onehot = to_categorical(y_train)\n",
    "\n",
    "print('Despues: ', y_train_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17eb7edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lo mismo con el paquete de testeo\n",
    "\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28*28)\n",
    "\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "X_test /= 255\n",
    "\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e15d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Antes:   ', y_test.shape)\n",
    "\n",
    "y_test_onehot = to_categorical(y_test)\n",
    "\n",
    "print('Despues: ', y_test_onehot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc209ad",
   "metadata": {},
   "source": [
    "### Construcción de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f92a2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# red de 3 capas\n",
    "\n",
    "net = Network()                           # inicia una red vacia de capas, y ahora añade...\n",
    "\n",
    "\n",
    "net.add(Dense(28*28, 100))                # input_shape=(1, 28*28) ; output_shape=(1, 100)\n",
    "net.add(Activation(tanh, tanh_prime))\n",
    "\n",
    "net.add(Dense(100, 50))                   # input_shape=(1, 100) ; output_shape=(1, 50)\n",
    "net.add(Activation(tanh, tanh_prime))\n",
    "\n",
    "net.add(Dense(50, 10))                    # input_shape=(1, 50) ; output_shape=(1, 10)\n",
    "net.add(Activation(tanh, tanh_prime))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa43644",
   "metadata": {},
   "source": [
    "**Entrenamiento de la red**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5c61b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# se elige la función de pérdida, error cuadrático medio y su derivada\n",
    "net.use(mse, mse_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbd01ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# se entrena el modelo con N datos, 35 épocas y 0.1 de tasa de aprendizaje\n",
    "\n",
    "N = 5000\n",
    "\n",
    "net.fit(X_train[0:N], y_train_onehot[0:N], epochs=35, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f962dca6",
   "metadata": {},
   "source": [
    "**Testeo de la red**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf1839a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# primera foto del testeo\n",
    "\n",
    "plt.imshow(X_test[0].reshape((28, 28)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7060eb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# etiqueta de la primera foto\n",
    "\n",
    "y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c28bd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# etiqueta de la primera foto, one-hot\n",
    "\n",
    "y_test_onehot[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fd6041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sacando el indice del valor maximo me da la etiqueta real\n",
    "\n",
    "np.argmax(y_test_onehot[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafe8336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediccion de la primera foto ¿? - sale de minimizar un MSE, es continuo\n",
    "\n",
    "net.predict(X_test[0])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7080efd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# de nuevo, sacando el indice del valor maximo me da la etiqueta real\n",
    "\n",
    "pred = net.predict(X_test[0])[0][0]\n",
    "\n",
    "np.argmax(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faca5325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicciones\n",
    "\n",
    "y_hat = net.predict(X_test)   # salida directa de la red  \n",
    "\n",
    "y_pred = [np.argmax(e[0]) for e in y_hat]   # etiqueta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f8fc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa4588e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78210ddc",
   "metadata": {},
   "source": [
    "**Evaluando el modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c221de0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score as acc\n",
    "\n",
    "from sklearn.metrics import f1_score as f1\n",
    "\n",
    "from sklearn.metrics import confusion_matrix as cm\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f2a53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# acierto\n",
    "\n",
    "acc(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8c638c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 score\n",
    "\n",
    "f1(y_test, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa35d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb59c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matriz de confucion\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "ax=sns.heatmap(cm(y_test, y_pred)/cm(y_test, y_pred).sum(), annot=True)\n",
    "\n",
    "plt.title('Matriz confusion')\n",
    "plt.ylabel('Verdad')\n",
    "plt.xlabel('Prediccion')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f859a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual",
   "language": "python",
   "name": "virtual"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
